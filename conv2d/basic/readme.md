# **PhÃ¢n TÃ­ch Thuáº­t ToÃ¡n CNN Thá»§ CÃ´ng**

---

## **1ï¸âƒ£ Load Data & Preprocessing**
ğŸ“Œ **Chá»©c nÄƒng:**  
- Äá»c áº£nh tá»« thÆ° má»¥c, chuyá»ƒn thÃ nh áº£nh grayscale vÃ  resize.  
- Chuáº©n hÃ³a pixel vá» `[0, 1]` Ä‘á»ƒ tÄƒng hiá»‡u suáº¥t huáº¥n luyá»‡n.  
- Chia táº­p train/test theo tá»· lá»‡ `80/20`.  

ğŸ” **PhÃ¢n tÃ­ch thuáº­t toÃ¡n:**  
- Duyá»‡t qua thÆ° má»¥c chá»©a áº£nh Ä‘á»ƒ láº¥y danh sÃ¡ch nhÃ£n (`class_labels`).  
- Äá»c áº£nh, resize vá» kÃ­ch thÆ°á»›c `(img_size, img_size)`.  
- Biáº¿n Ä‘á»•i áº£nh thÃ nh máº£ng NumPy, cÃ¹ng vá»›i nhÃ£n sá»‘ hÃ³a (`label_idx`).  
- **Äá»™ phá»©c táº¡p:** \(O(N)\) (vá»›i \(N\) lÃ  sá»‘ lÆ°á»£ng áº£nh).  

---

## **2ï¸âƒ£ Forward Propagation (Lan truyá»n xuÃ´i)**
ğŸ“Œ **Chá»©c nÄƒng:**  
- TÃ­nh toÃ¡n tá»«ng lá»›p cá»§a máº¡ng nÆ¡-ron theo thá»© tá»±:  
  **Conv2D â†’ ReLU â†’ Conv2D â†’ ReLU â†’ Flatten â†’ Dense â†’ Softmax**  

ğŸ” **PhÃ¢n tÃ­ch thuáº­t toÃ¡n:**  
### **â¤ Convolution Layer**
```python
 def conv2d(X, kernel):
    h, w = X.shape
    kh, kw = kernel.shape
    out_h, out_w = h - kh + 1, w - kw + 1
    output = np.zeros((out_h, out_w))
    
    for i in range(out_h):
        for j in range(out_w):
            output[i, j] = np.sum(X[i:i+kh, j:j+kw] * kernel)
    
    return output
```
- **CÃ´ng thá»©c:**  
  \[
  y[i, j] = \sum_{m=0}^{kh-1} \sum_{n=0}^{kw-1} X[i+m, j+n] \cdot K[m, n]
  \]
- **Äá»™ phá»©c táº¡p:** \(O(H \cdot W \cdot kH \cdot kW)\)  
  (Vá»›i \(H, W\) lÃ  kÃ­ch thÆ°á»›c áº£nh, \(kH, kW\) lÃ  kÃ­ch thÆ°á»›c kernel)  
- **Cáº£i tiáº¿n:**  
  - DÃ¹ng **im2col + phÃ©p nhÃ¢n ma tráº­n** Ä‘á»ƒ tÄƒng tá»‘c.  

### **â¤ ReLU Activation**
```python
 def relu(X):
    return np.maximum(0, X)
```
- **CÃ´ng thá»©c:**  
  \[
  f(x) = \max(0, x)
  \]
- **Äá»™ phá»©c táº¡p:** \(O(N)\), cháº¡y trÃªn tá»«ng pháº§n tá»­.  

### **â¤ Fully Connected (Dense)**
```python
 def dense(X, weights, bias):
    return np.dot(X, weights) + bias
```
- **CÃ´ng thá»©c:**  
  \[
  y = XW + b
  \]
- **Äá»™ phá»©c táº¡p:** \(O(NM)\).  

### **â¤ Softmax + Loss Function**
```python
 def softmax(X):
    exp_X = np.exp(X - np.max(X))
    return exp_X / np.sum(exp_X)

def cross_entropy_loss(y_true, y_pred):
    return -np.log(y_pred[y_true])
```
- **Softmax** tÃ­nh xÃ¡c suáº¥t dá»± Ä‘oÃ¡n.  
- **Cross-Entropy Loss** Ä‘o Ä‘á»™ sai khÃ¡c giá»¯a dá»± Ä‘oÃ¡n vÃ  thá»±c táº¿.  
- **Äá»™ phá»©c táº¡p:** \(O(N)\).  

---

## **3ï¸âƒ£ Backpropagation (Lan truyá»n ngÆ°á»£c)**
ğŸ“Œ **Chá»©c nÄƒng:**  
- TÃ­nh toÃ¡n gradient cá»§a tá»«ng lá»›p vÃ  cáº­p nháº­t trá»ng sá»‘.  

ğŸ” **PhÃ¢n tÃ­ch thuáº­t toÃ¡n:**  
### **â¤ Backpropagation trong Convolution**
```python
 def conv2d_backward(input, grad_output, kernel, lr):
    grad_kernel = np.zeros_like(kernel)
    for i in range(grad_output.shape[0]):
        for j in range(grad_output.shape[1]):
            grad_kernel += grad_output[i, j] * input[i:i+kernel.shape[0], j:j+kernel.shape[1]]
    kernel -= lr * grad_kernel
```
- **Gradient cá»§a Kernel:**  
  \[
  \frac{\partial L}{\partial K} = \sum \text{(Gradient Output) } \cdot \text{(Region Input)}
  \]
- **Äá»™ phá»©c táº¡p:** \(O(H \cdot W \cdot kH \cdot kW)\).  

---

## **4ï¸âƒ£ Training Loop**
ğŸ“Œ **Chá»©c nÄƒng:**  
- Láº·p `100` epochs.  
- Cáº­p nháº­t trá»ng sá»‘ sau má»—i batch size (`32` áº£nh).  
- TÃ­nh Accuracy & Loss sau má»—i epoch.  

ğŸ” **PhÃ¢n tÃ­ch thuáº­t toÃ¡n:**  
```python
 for epoch in range(100):
    total_loss = 0
    correct = 0
    for i in range(0, len(X_train), batch_size):
        batch_X = X_train[i:i+batch_size]
        batch_Y = Y_train[i:i+batch_size]
        
        # Forward Pass
        ...
        
        # Backpropagation
        ...
        
    accuracy = correct / len(X_train)
    print(f"Epoch {epoch+1}, Loss: {total_loss/len(X_train)}, Accuracy: {accuracy}")
```
- **Äá»™ phá»©c táº¡p:** \(O(E \cdot \frac{N}{B} \cdot C)\)  
  (E: Epochs, N: Sá»‘ máº«u, B: Batch Size, C: Sá»‘ phÃ©p tÃ­nh cá»§a model).  

---

## **5ï¸âƒ£ ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t**
ğŸ“Œ **Quan sÃ¡t tá»« káº¿t quáº£ train:**  
- **Accuracy dáº§n tÄƒng**, nghÄ©a lÃ  mÃ´ hÃ¬nh Ä‘ang há»c tá»‘t.  
- **Loss khÃ´ng giáº£m Ä‘á»u**, cÃ³ thá»ƒ do:  
  - Learning rate cáº§n giáº£m dáº§n (schedule).  
  - Dá»¯ liá»‡u chÆ°a Ä‘á»§ lá»›n.  
  - Model cÃ²n Ä‘Æ¡n giáº£n, cáº§n nhiá»u layers hÆ¡n.  

ğŸ“Œ **Gá»£i Ã½ cáº£i thiá»‡n:**  
- **ThÃªm Batch Normalization** sau má»—i Conv2D.  
- **Sá»­ dá»¥ng Adam Optimizer** thay vÃ¬ SGD.  
- **TÄƒng sá»‘ filters trong Convolution** Ä‘á»ƒ há»c Ä‘áº·c trÆ°ng tá»‘t hÆ¡n.  

---

## **ğŸ”¥ Káº¿t luáº­n**
- **Máº¡ng CNN thá»§ cÃ´ng nÃ y hoáº¡t Ä‘á»™ng tá»‘t nhÆ°ng cÃ²n Ä‘Æ¡n giáº£n.**  
- **CÃ³ thá»ƒ tá»‘i Æ°u tá»‘c Ä‘á»™ báº±ng im2col + GEMM** thay vÃ¬ vÃ²ng láº·p trong `conv2d`.  
- **MÃ´ hÃ¬nh cÃ³ thá»ƒ Ä‘áº¡t accuracy tá»‘t hÆ¡n vá»›i dropout + batchnorm.**  


